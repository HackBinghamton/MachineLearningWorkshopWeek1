{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks with Tensorflow\n",
    "## Overview\n",
    "\n",
    "### What You'll Learn\n",
    "In this section, you'll learn\n",
    "1. Why use neural networks for machine learning\n",
    "2. When to use neural networks\n",
    "3. How to implement a neural network with Tensorflow\n",
    "\n",
    "### Prerequisites\n",
    "Before starting this section, you should have an understanding of\n",
    "1. [scikit-learn](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshopWeek1/blob/master/intro_ml_scikit.ipynb)\n",
    "2. [Basic Python (functions, loops, lists)](https://github.com/HackBinghamton/PythonWorkshop)\n",
    "3. (Optional) [Matplotlib and numpy](https://github.com/HackBinghamton/DataScienceWorkshop)\n",
    "\n",
    "### Introduction\n",
    "Neural networks are a type of machine learning algorithm designed to make connections somewhat like the human brain does. They can be significantly more powerful than scikit-learn libraries, but aren't always the best approach to a machine learning problem.\n",
    "\n",
    "### Initial Setup Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "!pip3 install tensorflow\n",
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Neural Networks?\n",
    "\n",
    "So why use Neural Networks at all when we have such great models like Linear and Logistic Regression at our disposal?\n",
    "\n",
    "Well sometimes, these models just  aren't enough to separate out our data. Take this dataset for example:\n",
    "\n",
    "![Difficult data](images/gaussian-kernel.png)\n",
    "\n",
    "We can't just draw a straight line, or an S-shaped line though this thing to cleanly\n",
    "divide the red and blue points. We need a model that can take on more non-linear shapes.\n",
    "That's where Neural Networks can be very useful!\n",
    "\n",
    "> **NOTE**: Neural Networks can still be overkill for problems like this, but this is for the sake\n",
    "of an introductory workshop. There are plenty of other models in the Scikit Learn library that\n",
    "can handle data like this. Try checking out [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)\n",
    "for example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset\n",
    "We were able to achieve a respectable accuracy on scikit-learn's `digits` dataset using logistic regression. However,\n",
    "`digits` is quite small and simple - each image was only 8 pixels by 8 pixels. Modern computer monitors display millions of pixels at a time, so an 8x8 image is primitive compared to real-world data we'd be working with.\n",
    "\n",
    "The MNIST dataset, on the other hand, consists of 28 x 28 pixel images of handwritten digits for classification. This is closer to a real-world dataset's complexity, but this also means that our logistic regression approach from Part 1 will see a noticeable decrease in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Regressions with the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the MNIST Dataset\n",
    "First, we'll import tensorflow and load the MNIST dataset into our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load in the MNIST dataset from TensorFlow\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to split our data into training and testing sets. Unlike the `digits` dataset, the MNIST dataset is already split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning algorithms work best with values between 0 and 1, inclusive. However, each pixel value in the MNIST dataset is between 0 and 255. Let's fix this by dividing each pixel value by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "X_train, X_test = X_train / 255, X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the MNIST dataset is loaded in as a 60000 x 28 x 28 array. This is because there are 60000 images of size 28 x 28 pixels. However, Logistic Regression requires a two dimensional input! We need to vectorize our data by reshaping our data from three dimensions to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Logistic Regression Model on the MNIST Dataset\n",
    "As we did in Part 1, we'll now use `scikit-learn` to implement a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a LogisticRegression object\n",
    "logistic_model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "\n",
    "# Fit the logistic regression algorithm with the training data\n",
    "logistic_model.fit(X_train[:10000, :], Y_train[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it holds up -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "print(\"Logistic Regression Regression accuracy:\", str(logistic_model.score(X_test, Y_test) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While logistic regression achieved ~95% accuracy on the digits dataset, it only achieves ~90% on the MNIST dataset.\n",
    "But what if we need better than this? This is where neural networks come in handy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Our First Neural Network\n",
    "### Reloading the MNIST data\n",
    "Unlike scikit-learn's Logistic Regression model, Tensorflow's neural networks can process 3D data. Let's load the MNIST dataset back as a 3D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data() \n",
    "X_train, X_test = X_train / 255, X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Vectorize the input for faster processing\n",
    "'''\n",
    "flatten_layer = tf.keras.layers.Flatten(input_shape=(28, 28))\n",
    "\n",
    "\n",
    "'''\n",
    "    This line adds a dense layer to the neural network. Dense layers are fully connected layers, \n",
    "    which are the standard \"layers\" in a neural network.\n",
    "\n",
    "    128 - units of output dimensionality. We generally try to use powers of 2 (64, 128, 256, etc) \n",
    "    here because they're most efficient on GPUs. Finding a good value here is important - \n",
    "    2048 would be overkill on the MNIST dataset, but 16 might not be enough.\n",
    "\n",
    "    activation='relu' - relu stands for Rectified Linear Unit. Essentially, this activation adds \n",
    "    non-linearity to the neural network. If you try to run a linear regression model on this dataset, \n",
    "    you'll see it does very poorly. This suggests that it would be a good idea to add some \n",
    "    non-linearity to this problem.\n",
    "'''\n",
    "dense_relu_layer = tf.keras.layers.Dense(128, activation='relu')\n",
    "\n",
    "\n",
    "'''\n",
    "    Dropout is a good layer for avoiding overfitting - training a machine learning algorithm\n",
    "    on a training set too much. This causes the machine learning algorithm to notice irrelevant aspects \n",
    "    (\"noise\") of the training set. It essentially adds a layer of randomness to the neural network by \n",
    "    ignoring a percentage of random inputs (in this case, ignore a random 20%) on each iteration. \n",
    "\n",
    "    \"If you're good at something while drunk, you'll be really good at it sober\" - Ryan McCormick, former HackBU co-director\n",
    "'''\n",
    "dropout_layer = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "\n",
    "'''\n",
    "    Add another dense layer, but this time with an output dimensionality of 10 units because there are\n",
    "    only 10 options (there are only 10 digits).\n",
    "\n",
    "    activation='softmax' turns the arbitrary outputs of the neural network into \"probabilities\".\n",
    "\n",
    "    The final decision made by the neural network should be the output with the highest probability.\n",
    "\n",
    "    Example:\n",
    "        Input: Image of handwritten \"2\"\n",
    "        Output: [0.01, 0.02, 0.9, 0.01, 0.005, 0.005, 0.02, 0.01, 0.01, 0.01]\n",
    "            > Softmax gives us 10 probabilities, one for each digit 0 - 9.\n",
    "        Interpretation: This image is a handwritten \"2\" with probability of 0.9, or 90%.\n",
    "'''\n",
    "dense_softmax_layer = tf.keras.layers.Dense(10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    flatten_layer,\n",
    "    dense_relu_layer,\n",
    "    dropout_layer,\n",
    "    dense_softmax_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compile our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and fit it to our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "model.fit(X_train, Y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`epochs` is the number of times we fit our neural network to the training set. This value needs to be carefully decided in a neural network. \n",
    "\n",
    "While it may be tempting to make `epochs` as high as possible, too many epochs will cause our neural network to make irrelevant connections, decreasing its accuracy. This is known as **overfitting**.\n",
    "\n",
    "We all have that one friend that reads too much into 1 or 2 events and does something stupid as a result. Don't let your neural networks be that friend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "# Evaluate the accuracy of the neural network and print it out\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our simple neural network greatly outperforms our logistic regression model (~98% vs ~90%). Although an 8% increase may seem somewhat underwhelming, the accuracy increase vs. difficulty scale of neural networks is very much logarithmic. That is, it's often easier to go from 60% to 70% accuracy than it is to go from 98 to 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next section (recommended): [House Price Prediction with Machine Learning](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshopWeek1/blob/master/boston_housing_price_exercise.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
