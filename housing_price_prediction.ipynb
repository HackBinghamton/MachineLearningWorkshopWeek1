{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction with Machine Learning\n",
    "\n",
    "## Overview\n",
    "### What You'll Learn\n",
    "In this section, you'll learn\n",
    "1. How to use scikit-learn to create, train, and test a housing price predictor\n",
    "2. How to use Tensorflow to create, train, and test a neural\n",
    "\n",
    "### Prerequisites\n",
    "Before starting this section, you should have an understanding of\n",
    "1. [Basic Python](https://github.com/HackBinghamton/PythonWorkshop) (functions, loops, lists) \n",
    "2. [scikit-learn](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshopWeek1/blob/master/intro_ml_scikit.ipynb)\n",
    "3. [Tensorflow](https://colab.research.google.com/github/HackBinghamton/MachineLearningWorkshopWeek1/blob/master/intro_neural_networks_tf.ipynb)\n",
    "\n",
    "### Introduction\n",
    "This section will give you the opportunity to apply what you've learned with scikit-learn and Tensorflow to a new dataset: the Boston housing price dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Prediction with scikit-learn\n",
    "\n",
    "### 1. Loading the Data\n",
    "\n",
    "The Boston housing price dataset is one of several datasets included with `sklearn`. It contains 506 samples of houses in the Boston area, with measurements of 13 attributes of each (e.g. per capita crime, tax rate, pupil-teacher ratio, etc.), with the 'target' (y) variable being the price of the house. The goal is to train a model to find a regression from the x-data to the y-data.\n",
    "\n",
    "Accessing the data in the Boston house-price dataset is effectively the same as accessing the MNIST `digits` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(boston.data,\n",
    "                                                boston.target,\n",
    "                                                test_size=0.5,    # Tweak to your liking\n",
    "                                                random_state=42)  # Set random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can come up with a model that will accurately predict the price of a house given its attributes...\n",
    "\n",
    "### 2. Choosing a Model\n",
    "`sklearn` comes with a variety of models that excel at different tasks. \n",
    "\n",
    "There is one large distinction between models to make: There are *classifiers*, and *regressions*. __Classifiers pick from a list of label options to predict what something is (e.g. apples, oranges), while regressions guess a value on a continuous spectrum (e.g. a number on the Richter scale, *the price of a house*)__\n",
    "\n",
    " - __Classifiers *label* things, e.g.:__\n",
    "    - This is picture of a cat\n",
    "    - This data seems to represent an orange\n",
    "    - This sounds like this person's voice\n",
    " - __Regressions *estimate* things, e.g.:__\n",
    "    - This was probably a 3.5 on the Richter scale\n",
    "    - This stock will grow 5% by tomorrow\n",
    "    - This house probably cost $100k\n",
    "\n",
    "For this dataset, it is most valid to use a *regression* to predict the prices of the houses.\n",
    "\n",
    "`sklearn`'s `linear_model` family comes with numerous regressions to apply, like `LinearRegression`, `Lasso`, `Ridge`, and many more, which can be found at the [official documentation](https://scikit-learn.org/stable/supervised_learning.html).\n",
    "\n",
    "### 3. Training and Using a Model\n",
    "\n",
    "Thankfully, many of `sklearn`'s models work the same, and can be used by replacing `some_model_variety` below with the model family you'd like to load, and `ModelOfChoice` with the specific model you'd like to use.\n",
    "\n",
    "```python\n",
    "# Load up the model to use\n",
    "from sklearn.some_model_variety import ModelOfChoice\n",
    "\n",
    "# Load your data as shown above...\n",
    "\n",
    "# Create your model\n",
    "model = ModelOfChoice()\n",
    "\n",
    "# Train your model\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Check its accuracy\n",
    "print(\"ModelofChoice Accuracy:\", str(model.score(xtest, ytest) * 100) + \"%\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this template code below, see what you can find! What models work the best for this dataset? What can you tweak to get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.406624750749%\n"
     ]
    }
   ],
   "source": [
    "# Import model (also try Lasso, Ridge)\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create the model\n",
    "model = Ridge()\n",
    "\n",
    "# Train it\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Check the accuracy (if that is your intention)\n",
    "print(\"Accuracy:\", str(model.score(xtest, ytest) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You just used ML to make a model to predict housing prices!\n",
    "\n",
    "---\n",
    "\n",
    "## Prediction with Tensorflow\n",
    "\n",
    "### 1. Loading the Data\n",
    "\n",
    "Thankfully, Tensorflow comes with its own version of the Boston housing dataset, which can be accessed similarly to the MNIST set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "boston = tf.keras.datasets.boston_housing\n",
    "\n",
    "(xtrain, ytrain), (xtest, ytest) = boston.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing the Data\n",
    "\n",
    "Due to the math behind Tensorflow, the ranges and average values of different x-variables (e.g. crime rate vs. average rooms per house) have an impact on what the model considers important. For example, crime rates are formatted as percentages between 0 and 1, while the property tax is a number that usually lies in the 200s. Because of this difference, a neural network may consider property tax to be more hundreds of times more important than the crime rate.\n",
    "\n",
    "To take care of this, we must *normalize* our data -- effectively, this means to tweak them so that they sit on the same scale. One common way to do this is with *z-scores* (ignore the name, it just means to normalize).\n",
    "\n",
    "For each variable in our data, we'll perform the following operation:\n",
    "\n",
    "```x = (x - avg) / stddev```\n",
    "\n",
    "Thankfully, Tensorflow's data is lovely to work with and takes away much of the busywork for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = xtrain.mean(axis=0)\n",
    "train_stddev = xtrain.std(axis=0)\n",
    "xtrain = (xtrain - train_mean) / train_stddev\n",
    "xtest = (xtest - train_mean) / train_stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building the Model\n",
    "\n",
    "The neural network we construct here is remarkably similar to the one in the Intro Tensorflow section, with a couple of marked differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # (No need to vectorize our input with Flatten -- this is already taken care of.)\n",
    "    # (However, we still need to specify our input shape in the first layer...)\n",
    "    \n",
    "    # A Dense layer with relu, with the input shape tweaked to fit the 13 inputs of this dataset.\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(13,)),\n",
    "    \n",
    "    # A Dropout layer to prevent overfitting.\n",
    "    tf.keras.layers.Dropout(0.02),\n",
    "    \n",
    "    # Since we only have one dimension in our output (housing price) we only need 1 in our final Dense layer.\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compiling and Training the Model\n",
    "\n",
    "We can compile our model just like before, but with the subtraction of the accuracy metric -- this is because the accuracy used by Tensorflow doesn't make sense for continuous values.\n",
    "\n",
    "Also, we modify the loss function from `sparse_categorical_crossentropy` to `mean_squared_error` since  `sparse_categorical_crossentropy` is for categorization problems, and this is a regression.\n",
    "\n",
    "Finally, we train just like usual. Just be careful with the epochs to avoid overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/60\n",
      "404/404 [==============================] - 0s 873us/sample - loss: 549.8800\n",
      "Epoch 2/60\n",
      "404/404 [==============================] - 0s 51us/sample - loss: 526.8030\n",
      "Epoch 3/60\n",
      "404/404 [==============================] - 0s 51us/sample - loss: 503.8664\n",
      "Epoch 4/60\n",
      "404/404 [==============================] - 0s 57us/sample - loss: 479.5338\n",
      "Epoch 5/60\n",
      "404/404 [==============================] - 0s 56us/sample - loss: 454.6018\n",
      "Epoch 6/60\n",
      "404/404 [==============================] - 0s 52us/sample - loss: 428.7218\n",
      "Epoch 7/60\n",
      "404/404 [==============================] - 0s 52us/sample - loss: 399.7194\n",
      "Epoch 8/60\n",
      "404/404 [==============================] - 0s 51us/sample - loss: 370.4069\n",
      "Epoch 9/60\n",
      "404/404 [==============================] - 0s 52us/sample - loss: 340.4109\n",
      "Epoch 10/60\n",
      "404/404 [==============================] - 0s 51us/sample - loss: 309.2785\n",
      "Epoch 11/60\n",
      "404/404 [==============================] - 0s 51us/sample - loss: 279.8245\n",
      "Epoch 12/60\n",
      "404/404 [==============================] - 0s 52us/sample - loss: 250.9146\n",
      "Epoch 13/60\n",
      "404/404 [==============================] - 0s 52us/sample - loss: 225.6618\n",
      "Epoch 14/60\n",
      "404/404 [==============================] - 0s 52us/sample - loss: 198.3853\n",
      "Epoch 15/60\n",
      "404/404 [==============================] - 0s 54us/sample - loss: 177.3473\n",
      "Epoch 16/60\n",
      "404/404 [==============================] - 0s 52us/sample - loss: 157.0177\n",
      "Epoch 17/60\n",
      "404/404 [==============================] - 0s 53us/sample - loss: 138.7097\n",
      "Epoch 18/60\n",
      "404/404 [==============================] - 0s 58us/sample - loss: 123.5731\n",
      "Epoch 19/60\n",
      "404/404 [==============================] - 0s 52us/sample - loss: 111.0624\n",
      "Epoch 20/60\n",
      "404/404 [==============================] - 0s 55us/sample - loss: 100.0970\n",
      "Epoch 21/60\n",
      "404/404 [==============================] - 0s 57us/sample - loss: 90.7829\n",
      "Epoch 22/60\n",
      "404/404 [==============================] - 0s 83us/sample - loss: 81.7643\n",
      "Epoch 23/60\n",
      "404/404 [==============================] - 0s 81us/sample - loss: 75.2678\n",
      "Epoch 24/60\n",
      "404/404 [==============================] - 0s 58us/sample - loss: 67.7129\n",
      "Epoch 25/60\n",
      "404/404 [==============================] - 0s 64us/sample - loss: 61.5797\n",
      "Epoch 26/60\n",
      "404/404 [==============================] - 0s 54us/sample - loss: 56.5089\n",
      "Epoch 27/60\n",
      "404/404 [==============================] - 0s 54us/sample - loss: 51.7018\n",
      "Epoch 28/60\n",
      "404/404 [==============================] - 0s 53us/sample - loss: 48.4742\n",
      "Epoch 29/60\n",
      "404/404 [==============================] - 0s 51us/sample - loss: 44.5702\n",
      "Epoch 30/60\n",
      "404/404 [==============================] - 0s 60us/sample - loss: 42.0546\n",
      "Epoch 31/60\n",
      "404/404 [==============================] - 0s 51us/sample - loss: 38.5913\n",
      "Epoch 32/60\n",
      "404/404 [==============================] - 0s 53us/sample - loss: 37.8897\n",
      "Epoch 33/60\n",
      "404/404 [==============================] - 0s 53us/sample - loss: 35.1379\n",
      "Epoch 34/60\n",
      "404/404 [==============================] - 0s 54us/sample - loss: 34.4403\n",
      "Epoch 35/60\n",
      "404/404 [==============================] - 0s 52us/sample - loss: 32.3284\n",
      "Epoch 36/60\n",
      "404/404 [==============================] - 0s 54us/sample - loss: 31.1364\n",
      "Epoch 37/60\n",
      "404/404 [==============================] - 0s 66us/sample - loss: 30.8456\n",
      "Epoch 38/60\n",
      "404/404 [==============================] - 0s 86us/sample - loss: 29.6545\n",
      "Epoch 39/60\n",
      "404/404 [==============================] - 0s 55us/sample - loss: 29.0284\n",
      "Epoch 40/60\n",
      "404/404 [==============================] - 0s 54us/sample - loss: 28.3618\n",
      "Epoch 41/60\n",
      "404/404 [==============================] - 0s 65us/sample - loss: 27.8780\n",
      "Epoch 42/60\n",
      "404/404 [==============================] - 0s 64us/sample - loss: 27.0523\n",
      "Epoch 43/60\n",
      "404/404 [==============================] - 0s 55us/sample - loss: 26.3178\n",
      "Epoch 44/60\n",
      "404/404 [==============================] - 0s 63us/sample - loss: 25.8844\n",
      "Epoch 45/60\n",
      "404/404 [==============================] - 0s 57us/sample - loss: 26.2014\n",
      "Epoch 46/60\n",
      "404/404 [==============================] - 0s 55us/sample - loss: 25.0548\n",
      "Epoch 47/60\n",
      "404/404 [==============================] - 0s 57us/sample - loss: 25.2678\n",
      "Epoch 48/60\n",
      "404/404 [==============================] - 0s 70us/sample - loss: 24.9457\n",
      "Epoch 49/60\n",
      "404/404 [==============================] - 0s 62us/sample - loss: 24.2571\n",
      "Epoch 50/60\n",
      "404/404 [==============================] - 0s 82us/sample - loss: 23.3354\n",
      "Epoch 51/60\n",
      "404/404 [==============================] - 0s 79us/sample - loss: 24.1691\n",
      "Epoch 52/60\n",
      "404/404 [==============================] - 0s 72us/sample - loss: 23.7668\n",
      "Epoch 53/60\n",
      "404/404 [==============================] - 0s 62us/sample - loss: 22.2073\n",
      "Epoch 54/60\n",
      "404/404 [==============================] - 0s 57us/sample - loss: 22.5641\n",
      "Epoch 55/60\n",
      "404/404 [==============================] - 0s 55us/sample - loss: 22.2642\n",
      "Epoch 56/60\n",
      "404/404 [==============================] - 0s 58us/sample - loss: 22.3321\n",
      "Epoch 57/60\n",
      "404/404 [==============================] - 0s 53us/sample - loss: 21.6881\n",
      "Epoch 58/60\n",
      "404/404 [==============================] - 0s 56us/sample - loss: 21.4610\n",
      "Epoch 59/60\n",
      "404/404 [==============================] - 0s 70us/sample - loss: 21.0830\n",
      "Epoch 60/60\n",
      "404/404 [==============================] - 0s 57us/sample - loss: 21.1539\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs=60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing the Model\n",
    "\n",
    "Now that we've trained our model, let's test it out on some data! Run the code below to see the predicted and actual \n",
    "values of the test data with accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict  Actual   Accuracy\n",
      " 9.86     7.20    63.11\n",
      "17.45    18.80    92.81\n",
      "20.42    19.00    92.51\n",
      "33.09    27.00    77.43\n",
      "26.81    22.20    79.22\n",
      "15.80    24.50    64.48\n",
      "25.13    31.20    80.56\n",
      "23.37    22.90    97.95\n",
      "19.83    20.50    96.71\n",
      "17.29    23.20    74.54\n",
      " 9.37    18.60    50.40\n",
      "15.96    14.50    89.94\n",
      "18.57    17.80    95.67\n",
      "43.43    50.00    86.86\n",
      "17.08    20.80    82.14\n",
      "18.11    24.30    74.54\n",
      "25.15    24.20    96.08\n",
      "20.84    19.80    94.73\n",
      "15.97    19.10    83.60\n",
      "28.15    22.70    75.97\n",
      "13.17    12.00    90.24\n",
      "13.02    10.20    72.31\n",
      "19.56    20.00    97.82\n",
      "10.41    18.50    56.27\n",
      "24.62    20.90    82.18\n",
      "16.68    23.00    72.50\n",
      "30.95    27.50    87.46\n",
      "29.28    30.10    97.29\n",
      "12.66     9.50    66.69\n",
      "21.30    22.00    96.84\n",
      "19.96    21.20    94.14\n",
      "13.31    14.10    94.40\n",
      "30.28    33.10    91.49\n",
      "23.46    23.40    99.73\n",
      "14.16    20.10    70.45\n",
      "11.16     7.40    49.14\n",
      "13.59    15.40    88.22\n",
      "14.37    23.80    60.38\n",
      "22.62    20.10    87.45\n",
      "27.60    24.50    87.36\n",
      "30.60    33.00    92.73\n",
      "30.54    28.40    92.45\n",
      "17.23    14.10    77.79\n",
      "35.57    46.70    76.16\n",
      "35.06    32.50    92.13\n",
      "25.88    29.60    87.43\n",
      "26.30    28.40    92.59\n",
      "15.43    19.80    77.91\n",
      "19.95    20.20    98.78\n",
      "22.33    25.00    89.30\n",
      "32.76    35.40    92.54\n",
      "16.82    20.30    82.85\n",
      "15.12     9.70    44.11\n",
      "12.93    14.50    89.20\n",
      "31.72    34.90    90.90\n",
      "30.37    26.60    85.82\n",
      "16.09     7.20    -23.48\n",
      "44.35    50.00    88.70\n",
      "31.40    32.40    96.91\n",
      "26.90    21.60    75.44\n",
      "17.82    29.80    59.81\n",
      "18.54    13.10    58.45\n",
      "14.33    27.50    52.10\n",
      "17.53    21.20    82.70\n",
      "23.62    23.10    97.74\n",
      "26.14    21.90    80.63\n",
      "16.29    13.00    74.70\n",
      "24.59    23.20    93.99\n",
      "17.24     8.10    -12.79\n",
      "11.52     5.60    -5.64\n",
      "28.55    21.70    68.44\n",
      "26.30    29.60    88.85\n",
      "18.55    19.60    94.66\n",
      "19.49     7.00    -78.37\n",
      "24.50    26.40    92.81\n",
      "17.88    18.90    94.58\n",
      "20.59    20.90    98.53\n",
      "23.07    28.10    82.10\n",
      "36.08    35.40    98.07\n",
      "13.50    10.20    67.61\n",
      "18.61    24.30    76.57\n",
      "38.42    43.10    89.14\n",
      "11.90    17.60    67.64\n",
      "17.17    15.40    88.50\n",
      "18.23    16.20    87.49\n",
      "15.65    27.10    57.75\n",
      "17.48    21.40    81.68\n",
      "24.89    21.50    84.21\n",
      "24.78    22.40    89.37\n",
      "18.87    25.00    75.49\n",
      "18.68    16.60    87.47\n",
      "19.89    18.60    93.06\n",
      "25.50    22.00    84.09\n",
      "34.08    42.80    79.62\n",
      "36.14    35.10    97.03\n",
      "19.03    21.50    88.52\n",
      "36.97    36.00    97.30\n",
      "35.12    21.90    39.65\n",
      "23.13    24.10    95.96\n",
      "44.26    50.00    88.52\n",
      "32.85    26.70    76.97\n",
      "17.62    25.00    70.47\n",
      "Average Accuracy: 78.2094022568421%\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(xtest).flatten()\n",
    "results = ytest\n",
    "\n",
    "# Iterate through each test and pretty-print\n",
    "print(\"Predict  Actual   Accuracy\")\n",
    "sum_acc = 0\n",
    "for i in range(len(xtest)):\n",
    "    acc = 100 - (100 * abs(preds[i] - ytest[i]) / ytest[i])\n",
    "    sum_acc += acc\n",
    "    print(\"{:5.2f}    {:5.2f}    {:5.2f}\".format(preds[i], ytest[i], acc))\n",
    "\n",
    "print(\"Average Accuracy: {}%\".format(sum_acc / len(xtest)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
